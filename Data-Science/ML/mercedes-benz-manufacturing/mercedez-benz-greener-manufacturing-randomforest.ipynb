{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/mercedes-benz-greener-manufacturing/train.csv.zip\n/kaggle/input/mercedes-benz-greener-manufacturing/sample_submission.csv.zip\n/kaggle/input/mercedes-benz-greener-manufacturing/test.csv.zip\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Introduction\n\nLink to the Dataset: <a src=\"https://www.kaggle.com/c/mercedes-benz-greener-manufacturing\">https://www.kaggle.com/c/mercedes-benz-greener-manufacturing</a>\n\nSince the first automobile, the Benz Patent Motor Car in 1886, Mercedes-Benz has stood for important automotive innovations. These include, for example, the passenger safety cell with crumple zone, the airbag and intelligent assistance systems. Mercedes-Benz applies for nearly 2000 patents per year, making the brand the European leader among premium car makers. Daimler’s Mercedes-Benz cars are leaders in the premium car industry. With a huge selection of features and options, customers can choose the customized Mercedes-Benz of their dreams. .\n\nTo ensure the safety and reliability of each and every unique car configuration before they hit the road, Daimler’s engineers have developed a robust testing system. But, optimizing the speed of their testing system for so many possible feature combinations is complex and time-consuming without a powerful algorithmic approach. As one of the world’s biggest manufacturers of premium cars, safety and efficiency are paramount on Daimler’s production lines.\n\nIn this competition, Daimler is challenging Kagglers to tackle the curse of dimensionality and reduce the time that cars spend on the test bench. Competitors will work with a dataset representing different permutations of Mercedes-Benz car features to predict the time it takes to pass testing. Winning algorithms will contribute to speedier testing, resulting in lower carbon dioxide emissions without reducing Daimler’s standards."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\n# Set some options \npd.set_option('display.max_colwidth', 100)\nsns.set(style=\"whitegrid\", color_codes=True)\n\n# Set Matplotlib defaults\n%matplotlib inline","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/mercedes-benz-greener-manufacturing/train.csv.zip')\ntest_df = pd.read_csv('/kaggle/input/mercedes-benz-greener-manufacturing/test.csv.zip')\nprint(train_df.shape)\ntrain_df.head(5)","execution_count":140,"outputs":[{"output_type":"stream","text":"(4209, 378)\n","name":"stdout"},{"output_type":"execute_result","execution_count":140,"data":{"text/plain":"   ID       y  X0 X1  X2 X3 X4 X5 X6 X8  ...  X375  X376  X377  X378  X379  \\\n0   0  130.81   k  v  at  a  d  u  j  o  ...     0     0     1     0     0   \n1   6   88.53   k  t  av  e  d  y  l  o  ...     1     0     0     0     0   \n2   7   76.26  az  w   n  c  d  x  j  x  ...     0     0     0     0     0   \n3   9   80.62  az  t   n  f  d  x  l  e  ...     0     0     0     0     0   \n4  13   78.02  az  v   n  f  d  h  d  n  ...     0     0     0     0     0   \n\n   X380  X382  X383  X384  X385  \n0     0     0     0     0     0  \n1     0     0     0     0     0  \n2     0     1     0     0     0  \n3     0     0     0     0     0  \n4     0     0     0     0     0  \n\n[5 rows x 378 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>y</th>\n      <th>X0</th>\n      <th>X1</th>\n      <th>X2</th>\n      <th>X3</th>\n      <th>X4</th>\n      <th>X5</th>\n      <th>X6</th>\n      <th>X8</th>\n      <th>...</th>\n      <th>X375</th>\n      <th>X376</th>\n      <th>X377</th>\n      <th>X378</th>\n      <th>X379</th>\n      <th>X380</th>\n      <th>X382</th>\n      <th>X383</th>\n      <th>X384</th>\n      <th>X385</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>130.81</td>\n      <td>k</td>\n      <td>v</td>\n      <td>at</td>\n      <td>a</td>\n      <td>d</td>\n      <td>u</td>\n      <td>j</td>\n      <td>o</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6</td>\n      <td>88.53</td>\n      <td>k</td>\n      <td>t</td>\n      <td>av</td>\n      <td>e</td>\n      <td>d</td>\n      <td>y</td>\n      <td>l</td>\n      <td>o</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7</td>\n      <td>76.26</td>\n      <td>az</td>\n      <td>w</td>\n      <td>n</td>\n      <td>c</td>\n      <td>d</td>\n      <td>x</td>\n      <td>j</td>\n      <td>x</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n      <td>80.62</td>\n      <td>az</td>\n      <td>t</td>\n      <td>n</td>\n      <td>f</td>\n      <td>d</td>\n      <td>x</td>\n      <td>l</td>\n      <td>e</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>13</td>\n      <td>78.02</td>\n      <td>az</td>\n      <td>v</td>\n      <td>n</td>\n      <td>f</td>\n      <td>d</td>\n      <td>h</td>\n      <td>d</td>\n      <td>n</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 378 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.info()","execution_count":7,"outputs":[{"output_type":"stream","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 4209 entries, 0 to 4208\nColumns: 378 entries, ID to X385\ndtypes: float64(1), int64(369), object(8)\nmemory usage: 12.1+ MB\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.select_dtypes(include='float64').columns.to_list()","execution_count":77,"outputs":[{"output_type":"execute_result","execution_count":77,"data":{"text/plain":"['y']"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Only target column is of type float"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.describe()","execution_count":31,"outputs":[{"output_type":"execute_result","execution_count":31,"data":{"text/plain":"                ID            y          X10     X11          X12  \\\ncount  4209.000000  4209.000000  4209.000000  4209.0  4209.000000   \nmean   4205.960798   100.669318     0.013305     0.0     0.075077   \nstd    2437.608688    12.679381     0.114590     0.0     0.263547   \nmin       0.000000    72.110000     0.000000     0.0     0.000000   \n25%    2095.000000    90.820000     0.000000     0.0     0.000000   \n50%    4220.000000    99.150000     0.000000     0.0     0.000000   \n75%    6314.000000   109.010000     0.000000     0.0     0.000000   \nmax    8417.000000   265.320000     1.000000     0.0     1.000000   \n\n               X13          X14          X15          X16          X17  ...  \\\ncount  4209.000000  4209.000000  4209.000000  4209.000000  4209.000000  ...   \nmean      0.057971     0.428130     0.000475     0.002613     0.007603  ...   \nstd       0.233716     0.494867     0.021796     0.051061     0.086872  ...   \nmin       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n25%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n50%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n75%       0.000000     1.000000     0.000000     0.000000     0.000000  ...   \nmax       1.000000     1.000000     1.000000     1.000000     1.000000  ...   \n\n              X375         X376         X377         X378         X379  \\\ncount  4209.000000  4209.000000  4209.000000  4209.000000  4209.000000   \nmean      0.318841     0.057258     0.314802     0.020670     0.009503   \nstd       0.466082     0.232363     0.464492     0.142294     0.097033   \nmin       0.000000     0.000000     0.000000     0.000000     0.000000   \n25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n75%       1.000000     0.000000     1.000000     0.000000     0.000000   \nmax       1.000000     1.000000     1.000000     1.000000     1.000000   \n\n              X380         X382         X383         X384         X385  \ncount  4209.000000  4209.000000  4209.000000  4209.000000  4209.000000  \nmean      0.008078     0.007603     0.001663     0.000475     0.001426  \nstd       0.089524     0.086872     0.040752     0.021796     0.037734  \nmin       0.000000     0.000000     0.000000     0.000000     0.000000  \n25%       0.000000     0.000000     0.000000     0.000000     0.000000  \n50%       0.000000     0.000000     0.000000     0.000000     0.000000  \n75%       0.000000     0.000000     0.000000     0.000000     0.000000  \nmax       1.000000     1.000000     1.000000     1.000000     1.000000  \n\n[8 rows x 370 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>y</th>\n      <th>X10</th>\n      <th>X11</th>\n      <th>X12</th>\n      <th>X13</th>\n      <th>X14</th>\n      <th>X15</th>\n      <th>X16</th>\n      <th>X17</th>\n      <th>...</th>\n      <th>X375</th>\n      <th>X376</th>\n      <th>X377</th>\n      <th>X378</th>\n      <th>X379</th>\n      <th>X380</th>\n      <th>X382</th>\n      <th>X383</th>\n      <th>X384</th>\n      <th>X385</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>4209.000000</td>\n      <td>4209.000000</td>\n      <td>4209.000000</td>\n      <td>4209.0</td>\n      <td>4209.000000</td>\n      <td>4209.000000</td>\n      <td>4209.000000</td>\n      <td>4209.000000</td>\n      <td>4209.000000</td>\n      <td>4209.000000</td>\n      <td>...</td>\n      <td>4209.000000</td>\n      <td>4209.000000</td>\n      <td>4209.000000</td>\n      <td>4209.000000</td>\n      <td>4209.000000</td>\n      <td>4209.000000</td>\n      <td>4209.000000</td>\n      <td>4209.000000</td>\n      <td>4209.000000</td>\n      <td>4209.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>4205.960798</td>\n      <td>100.669318</td>\n      <td>0.013305</td>\n      <td>0.0</td>\n      <td>0.075077</td>\n      <td>0.057971</td>\n      <td>0.428130</td>\n      <td>0.000475</td>\n      <td>0.002613</td>\n      <td>0.007603</td>\n      <td>...</td>\n      <td>0.318841</td>\n      <td>0.057258</td>\n      <td>0.314802</td>\n      <td>0.020670</td>\n      <td>0.009503</td>\n      <td>0.008078</td>\n      <td>0.007603</td>\n      <td>0.001663</td>\n      <td>0.000475</td>\n      <td>0.001426</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>2437.608688</td>\n      <td>12.679381</td>\n      <td>0.114590</td>\n      <td>0.0</td>\n      <td>0.263547</td>\n      <td>0.233716</td>\n      <td>0.494867</td>\n      <td>0.021796</td>\n      <td>0.051061</td>\n      <td>0.086872</td>\n      <td>...</td>\n      <td>0.466082</td>\n      <td>0.232363</td>\n      <td>0.464492</td>\n      <td>0.142294</td>\n      <td>0.097033</td>\n      <td>0.089524</td>\n      <td>0.086872</td>\n      <td>0.040752</td>\n      <td>0.021796</td>\n      <td>0.037734</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>72.110000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>2095.000000</td>\n      <td>90.820000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>4220.000000</td>\n      <td>99.150000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>6314.000000</td>\n      <td>109.010000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>8417.000000</td>\n      <td>265.320000</td>\n      <td>1.000000</td>\n      <td>0.0</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>...</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 370 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"#### Check the number of null columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.columns[train_df.isna().any()], test_df.columns[test_df.isna().any()]","execution_count":17,"outputs":[{"output_type":"execute_result","execution_count":17,"data":{"text/plain":"(Index([], dtype='object'), Index([], dtype='object'))"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_zero_var_cols(df):\n    zero_var_columns = df.var()\n    return zero_var_columns[zero_var_columns == 0].index.to_list()\n\ndef drop_zero_var_cols(df, threshold=0):\n    columns = get_zero_var_cols(df)\n    print(f\"Dropping columns : {columns}\")\n    return df.drop(get_zero_var_cols(df), axis=1)\n\ndef get_dtype_col_names(df, include='object'):\n    return df.select_dtypes(include=include).columns.to_list()\n\ndef get_uniq_label_counts(df):\n    object_columns = get_object_col_names(df)\n    return list(map(lambda x : (x, len(df[x].unique())), object_columns))\n\ndef get_col_top_labels(df, col, top=10):\n    index = df[col].value_counts().head(top).index\n    return index.to_list()\n\ndef top_one_hot(df):\n    object_columns = get_dtype_col_names(df)\n    one_hot_df = pd.DataFrame()\n    for col in object_columns:\n        # print(col, ':',  get_col_top_labels(df, col))\n        for label in get_col_top_labels(df, col):\n            one_hot_col = str(col + '_' + label)\n            series = df[col]\n            if one_hot_col not in one_hot_df.columns.to_list():\n                one_hot_df[one_hot_col] = np.where(series == label, 1, 0)\n                \n    return one_hot_df","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# List of columns to drop\ndrop_columns = get_zero_var_cols(train_df) # 0 Variance columne\ndrop_columns.append('ID')                  # id column\n\n# Take categorical columns\nobject_columns = get_dtype_col_names(train_df)\nnumerical_columns = get_dtype_col_names(train_df, include='int64')\n\nnumerical_columns = [value for value in numerical_columns if value not in drop_columns]\nprint(f\"Numerical Columns : {numerical_columns[:10]}\")\nprint(f\"Object Columns : {object_columns}\")\nprint(f\"{len(drop_columns)} columns will be dropped\")","execution_count":64,"outputs":[{"output_type":"stream","text":"Numerical Columns : ['X10', 'X12', 'X13', 'X14', 'X15', 'X16', 'X17', 'X18', 'X19', 'X20']\nObject Columns : ['X0', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X8']\n13 columns will be dropped\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.decomposition import PCA\n\nclass TopOneHotEncoder(BaseEstimator, TransformerMixin):\n    def get_col_top_labels(self, X, col):\n        index = X[col].value_counts().head(self.top).index\n        return index.to_list()\n    \n    def top_one_hot(self, X):\n        one_hot_df = pd.DataFrame()\n        for col in self._feature_columns:\n            for label in self.get_col_top_labels(X,col):\n                one_hot_col = str(col + '_' + label)\n                series = X[col]\n                if one_hot_col not in one_hot_df.columns.to_list():\n                    one_hot_df[one_hot_col] = np.where(series == label, 1, 0)\n\n        return one_hot_df    \n\n    def __init__(self, feature_columns, top=10):\n        self._feature_columns = feature_columns\n        self.top = top\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        df_one_hot = self.top_one_hot(X)\n#         X = X.drop(self._feature_columns, axis=1)\n        return df_one_hot\n\nclass SelectColumnTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, feature_columns):\n        self._feature_columns = feature_columns\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        return X.drop(self._feature_columns, axis=1)    \n    \nclass PCATransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, numerical_features, categorical_features, n_components=3):\n        self._numerical_features = numerical_features\n        self._categorical_features = categorical_features\n        self._n_components = n_components\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        \n        X_new, y = X.drop('y', axis=1), X['y']\n        \n        one_hot = TopOneHotEncoder(feature_columns=self._categorical_features)\n        df_one_hot = one_hot.transform(X_new[self._categorical_features])\n        \n        pca = PCA(n_componetns=self._n_components)\n        X_pca = pca.transform(X[self._numerical_features])\n        print(type(X_pca))\n        print(type(df_one_hot))\n        return X_pca + df_one_hot.values, y\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\npreprocess_pipe = Pipeline([('drop_columns', SelectColumnTransformer(drop_columns)),\n                            ('scaler',StandardScaler()),\n                            ('pca',PCATransformer(numerical_features=numerical_columns,\n                                                            categorical_features=object_columns,\n                                                            n_components=30)),\n                            ])    ","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocessing(df,drop_columns, numerical_columns, categorical_columns, n_components=30):\n    select = SelectColumnTransformer(drop_columns)\n    df = select.transform(df)\n    \n    X = df[numerical_columns]\n    \n    # Scale the data\n    from sklearn.preprocessing import StandardScaler\n    scaler = StandardScaler()\n    X = scaler.fit_transform(X)\n    \n    # Perform PCA\n    from sklearn.decomposition import PCA\n    pca = PCA(n_components=n_components)\n    X = pca.fit_transform(X)\n    \n    # Do Top One-Hot encoding\n    one_hot = TopOneHotEncoder(feature_columns=categorical_columns)\n    one_hot_df = one_hot.transform(df[categorical_columns])\n    print(f\"Top One Hot Encoded columns : {one_hot_df.shape[1]}\")\n    X = np.append(X, one_hot_df.to_numpy(), axis = 1)\n    print(f\"PCA n_components : {n_components}\")\n    print(f\"Total columns {X.shape[1]}\")\n    return X\n    ","execution_count":148,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = train_df['y']\nX_train = preprocessing(train_df, drop_columns, numerical_columns, object_columns, 30)\nX_test = preprocessing(train_df, drop_columns, numerical_columns, object_columns, 30)","execution_count":150,"outputs":[{"output_type":"stream","text":"Top One Hot Encoded columns : 71\nPCA n_components : 30\nTotal columns 101\nTop One Hot Encoded columns : 71\nPCA n_components : 30\nTotal columns 101\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fine_tune_model(model, X_train, y_train, param_grid,\n                    cv=5, verbose=0, scoring=None):\n    from sklearn.model_selection import GridSearchCV\n    grid_search = GridSearchCV(model, param_grid, cv=cv, verbose=verbose, n_jobs=-1)\n    grid_search.fit(X_train, y_train)\n    \n    print(f\"Performing Grid search for {type(model).__name__} ...\")\n    print(f\"Best parameters : {grid_search.best_params_} with score {grid_search.best_score_:.2}\")\n#     model.fit(X_train, y_train)\n    \n#     y_pred = model.predict(X_test)\n#     from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n#     from sklearn.metrics import mean_squared_error\n#     import sklearn.metrics as sm\n#     print(\"Mean absolute error =\", round(sm.mean_absolute_error(y_test, y_pred), 2)) \n#     print(\"Mean squared error =\", round(sm.mean_squared_error(y_test, y_pred), 2)) \n#     print(\"Median absolute error =\", round(sm.median_absolute_error(y_test, y_pred), 2)) \n#     print(\"Explain variance score =\", round(sm.explained_variance_score(y_test, y_pred), 2)) \n#     print(\"R2 score =\", round(sm.r2_score(y_test, y_pred), 2))\n    return grid_search.best_estimator_\n#     lin_mse = mean_squared_error(y_test, y_pred)\n#     lin_rmse = np.sqrt(lin_mse)\n#     cnf =  confusion_matrix(y_test, y_pred)\n#     report = classification_report(y_test, y_pred)\n#     accuracy = accuracy_score(y_test, y_pred)\n#     print(f\"\\n{cnf}\\n{report}\\n\")\n#     print\n#     print(f\"Accuracy of Model : {accuracy:.2}\")    ","execution_count":151,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nfrom sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor()\nparam_grid = [{'n_estimators': range(30,100,5), 'max_features': range(6,24,3)},\n              {'bootstrap': [False], 'n_estimators': range(30,100,5), 'max_features': range(6,24,3)}]\nfine_tune_model(model,X_train, y_train, param_grid)","execution_count":139,"outputs":[{"output_type":"stream","text":"Performing Grid search for RandomForestRegressor ...\nBest parameters : {'max_features': 21, 'n_estimators': 75} with score 0.48\nMean absolute error = 3.22\nMean squared error = 25.55\nMedian absolute error = 2.21\nExplain variance score = 0.84\nR2 score = 0.84\nCPU times: user 21.8 s, sys: 295 ms, total: 22.1 s\nWall time: 8min 14s\n","name":"stdout"},{"output_type":"execute_result","execution_count":139,"data":{"text/plain":"RandomForestRegressor(max_features=21, n_estimators=75)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nfrom sklearn.tree import DecisionTreeRegressor \nfrom sklearn.ensemble import AdaBoostRegressor\n\nparam_grid = [{'n_estimators': [50, 70, 100], 'learning_rate': [0.1, 0.2, 0.5]},\n              {'loss': ['linear', 'square', 'exponential']}]\n\nmodel = AdaBoostRegressor()\nfine_tune_model(model,X_train, y_train, param_grid)","execution_count":115,"outputs":[{"output_type":"stream","text":"Performing Grid search for AdaBoostRegressor ...\nBest parameters : {'learning_rate': 0.1, 'n_estimators': 50} with score 0.27\nMean absolute error = 10.46\nMean squared error = 148.89\nMedian absolute error = 10.39\nExplain variance score = 0.38\nR2 score = 0.07\nCPU times: user 3.51 s, sys: 69 ms, total: 3.58 s\nWall time: 46.5 s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.tree import DecisionTreeRegressor \n\nparam_grid = [{'n_estimators': [50, 70, 100], 'learning_rate': [0.1, 0.2, 0.5]},\n              {'loss': ['ls', 'lad', 'huber']}]\n\nmodel = GradientBoostingRegressor()\nfine_tune_model(model,X_train, y_train, param_grid)","execution_count":116,"outputs":[{"output_type":"stream","text":"Performing Grid search for GradientBoostingRegressor ...\nBest parameters : {'loss': 'huber'} with score 0.5\nMean absolute error = 5.43\nMean squared error = 63.85\nMedian absolute error = 4.09\nExplain variance score = 0.6\nR2 score = 0.6\nCPU times: user 9.32 s, sys: 28 ms, total: 9.35 s\nWall time: 1min 13s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nimport xgboost as xg \n\n# Train and test set are converted to DMatrix objects, as it is required by learning API. \ntrain_dmatrix = xg.DMatrix(data = X_train, label = y_train) \ntest_dmatrix = xg.DMatrix(data = X_test, label = y_test) \n  \n# Parameter dictionary specifying base learner \nparam = {\"booster\":\"gblinear\", \"objective\":\"reg:squarederror\"} \n  \nxgb_r = xg.train(params = param, dtrain = train_dmatrix, num_boost_round = 10) \npred = xgb_r.predict(test_dmatrix) \n# fine_tune_model(model,X_train, y_train, param_grid)\n\n# RMSE Computation \nimport sklearn.metrics as sm\nprint(\"Mean absolute error =\", round(sm.mean_absolute_error(y_test, pred), 2)) \nprint(\"Mean squared error =\", round(sm.mean_squared_error(y_test, pred), 2)) \nprint(\"Median absolute error =\", round(sm.median_absolute_error(y_test, pred), 2)) \nprint(\"Explain variance score =\", round(sm.explained_variance_score(y_test, pred), 2)) \nprint(\"R2 score =\", round(sm.r2_score(y_test, pred), 2))\n","execution_count":122,"outputs":[{"output_type":"stream","text":"Mean absolute error = 6.54\nMean squared error = 89.37\nMedian absolute error = 5.1\nExplain variance score = 0.44\nR2 score = 0.44\nCPU times: user 139 ms, sys: 3.09 ms, total: 142 ms\nWall time: 37.7 ms\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Best performance was given by Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = [{'n_estimators': range(30,100,5), 'max_features': range(6,24,3)},\n              {'bootstrap': [False], 'n_estimators': range(30,100,5), 'max_features': range(6,24,3)}]\nmodel = fine_tune_model(RandomForestRegressor(),X_train, y_train, param_grid)\n","execution_count":152,"outputs":[{"output_type":"stream","text":"Performing Grid search for RandomForestRegressor ...\nBest parameters : {'max_features': 21, 'n_estimators': 90} with score 0.47\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save data in a notebook\ndf_final = pd.DataFrame(test_df[['ID']])\ny_pred = model.predict(X_test)\ndf_final['y_pred'] = y_pred\n# df['mae'] = np.abs(df['y'] - df['y_pred'])\ndf_final.head()\ndf_final.to_csv('submission.csv')","execution_count":156,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}